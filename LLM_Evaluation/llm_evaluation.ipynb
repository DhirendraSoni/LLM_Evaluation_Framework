{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import Bedrock libraries (assumed SDK available)\n",
    "from boto3 import client, session\n",
    "import json\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 00:29:40.200 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.330 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\dhire\\Personal\\AI_ML\\Udemy\\DS_GenAI\\venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-01-18 00:29:40.330 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.342 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.344 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-18 00:29:40.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "class LLMComparisonFramework:\n",
    "    def __init__(self, models, dataset, max_summary_length=100):\n",
    "        st.write(\"Initializing the framework with models and dataset.\")\n",
    "        print(\"Initializing the framework with models and dataset.\")\n",
    "        self.models = models  # List of model names\n",
    "        self.dataset = dataset  # Dataset as a DataFrame with 'text' and 'reference_summary'\n",
    "        self.max_summary_length = max_summary_length\n",
    "        self.results = pd.DataFrame(columns=['Model', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'])\n",
    "        self.bedrock_client = self.initialize_bedrock_client()\n",
    "\n",
    "    def initialize_bedrock_client(self):\n",
    "        print(\"Initializing AWS Bedrock client.\")\n",
    "        st.write(\"Initializing AWS Bedrock client.\")\n",
    "        try:\n",
    "            aws_session = session.Session()\n",
    "            client = aws_session.client(\n",
    "                'bedrock-runtime',\n",
    "                region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"),\n",
    "                aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "                aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "            )\n",
    "            print(\"AWS Bedrock client initialized successfully.\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize AWS Bedrock client: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        st.write(f\"Loading model: {model_name}\")\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        if model_name.startswith(\"bedrock:\"):\n",
    "            return None, model_name  # Bedrock models do not use tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        return tokenizer, model\n",
    "\n",
    "    def generate_summary(self, model, tokenizer, text):\n",
    "        st.write(\"Generating summary for a given text.\")\n",
    "        print(\"Generating summary for a given text.\")\n",
    "        if isinstance(model, str) and model.startswith(\"bedrock:\"):\n",
    "            return self.generate_bedrock_summary(model, text)\n",
    "\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True)\n",
    "        outputs = model.generate(inputs, max_length=self.max_summary_length, num_beams=4)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def client(self, service_name):\n",
    "        st.write(f\"Creating client for service: {service_name}\")\n",
    "        print(f\"Creating client for service: {service_name}\")\n",
    "        import boto3\n",
    "        return boto3.client(service_name)\n",
    "\n",
    "    def generate_bedrock_summary(self, model_name, text):\n",
    "        st.write(f\"Generating summary using Amazon Bedrock model: {model_name}\")\n",
    "        print(f\"Generating summary using Amazon Bedrock model: {model_name}\")\n",
    "        model_id = model_name.split(\":\")[1]  # Extract Bedrock model ID\n",
    "        payload = {\n",
    "            \"inputText\": text,\n",
    "                    \"textGenerationConfig\": {\n",
    "                        \"maxTokenCount\": 4096,\n",
    "                        \"stopSequences\": [],\n",
    "                        \"temperature\": 0,\n",
    "                        \"topP\": 1\n",
    "                    }\n",
    "                    }\n",
    "        try:\n",
    "            response = self.bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "            st.write(\"Successfully generated summary using Bedrock.\")\n",
    "            print(\"Successfully generated summary using Bedrock.\")\n",
    "            return response[\"body\"].read().decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary with Bedrock model {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def calculate_rouge(self, predicted, reference):\n",
    "        st.write(\"Calculating ROUGE scores.\")\n",
    "        print(\"Calculating ROUGE scores.\")\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(reference, predicted)\n",
    "        return {\n",
    "            'ROUGE-1': scores['rouge1'].fmeasure,\n",
    "            'ROUGE-2': scores['rouge2'].fmeasure,\n",
    "            'ROUGE-L': scores['rougeL'].fmeasure\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, model_name):\n",
    "        st.write(f\"Evaluating model: {model_name}\")\n",
    "        print(f\"Evaluating model: {model_name}\")\n",
    "        tokenizer, model = self.load_model(model_name)\n",
    "        rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "        for _, row in self.dataset.iterrows():\n",
    "            text = row['text']\n",
    "            reference_summary = row['reference_summary']\n",
    "\n",
    "            predicted_summary = self.generate_summary(model, tokenizer, text)\n",
    "            rouge_scores = self.calculate_rouge(predicted_summary, reference_summary)\n",
    "\n",
    "            rouge1_scores.append(rouge_scores['ROUGE-1'])\n",
    "            rouge2_scores.append(rouge_scores['ROUGE-2'])\n",
    "            rougeL_scores.append(rouge_scores['ROUGE-L'])\n",
    "\n",
    "        new_result = {\n",
    "            'Model': model_name,\n",
    "            'ROUGE-1': sum(rouge1_scores) / len(rouge1_scores),\n",
    "            'ROUGE-2': sum(rouge2_scores) / len(rouge2_scores),\n",
    "            'ROUGE-L': sum(rougeL_scores) / len(rougeL_scores)\n",
    "        }\n",
    "        self.results = pd.concat([self.results, pd.DataFrame([new_result])], ignore_index=True)\n",
    "\n",
    "    def evaluate_all_models(self):\n",
    "        st.write(\"Evaluating all models on the dataset.\")\n",
    "        print(\"Evaluating all models on the dataset.\")\n",
    "        for model_name in self.models:\n",
    "            print(f\"Evaluating {model_name}...\")\n",
    "            self.evaluate_model(model_name)\n",
    "\n",
    "    def visualize_results(self):\n",
    "        print(\"Visualizing ROUGE scores for all models.\")\n",
    "        print(\"Visualization is not available as matplotlib is not installed.\")\n",
    "        print(self.results)\n",
    "\n",
    "    def deploy_best_model(self):\n",
    "        st.write(\"Selecting and deploying the best model based on ROUGE-L score.\")\n",
    "        print(\"Selecting and deploying the best model based on ROUGE-L score.\")\n",
    "        best_model_name = self.results.loc[self.results['ROUGE-L'].idxmax(), 'Model']\n",
    "        print(f\"Deploying best model: {best_model_name}\")\n",
    "        return self.load_model(best_model_name)\n",
    "\n",
    "# Example Usage => Streamlit UI\n",
    "if __name__ == \"__main__\":\n",
    "    st.title(\"LLM Comparison Framework\")\n",
    "\n",
    "    st.write(\"Upload your dataset with 'text' and 'reference_summary' columns.\")\n",
    "    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "\n",
    "    if uploaded_file:\n",
    "        dataset = pd.read_csv(uploaded_file)\n",
    "        st.write(\"Dataset loaded successfully.\")\n",
    "        st.write(dataset)\n",
    "\n",
    "        st.write(\"Enter model names to compare, separated by commas.\")\n",
    "        model_input = st.text_input(\"Model names\", \"facebook/bart-large-cnn, google/pegasus-xsum, bedrock:titan-embed-text-v2:0\")\n",
    "        models = [model.strip() for model in model_input.split(\",\")]\n",
    "\n",
    "        st.write(\"Initializing framework...\")\n",
    "        framework = LLMComparisonFramework(models, dataset)\n",
    "\n",
    "        if st.button(\"Evaluate Models\"):\n",
    "            framework.evaluate_all_models()\n",
    "            framework.visualize_results()\n",
    "\n",
    "        # if st.button(\"Deploy Best Model\"):\n",
    "        #     framework.deploy_best_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
