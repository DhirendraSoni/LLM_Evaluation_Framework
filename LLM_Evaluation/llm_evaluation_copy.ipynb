{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import Bedrock libraries (assumed SDK available)\n",
    "from boto3 import client, session\n",
    "import json\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating example dataset.\n",
      "Defining models to compare.\n",
      "Initializing LLMComparisonFramework.\n",
      "Initializing the framework with models and dataset.\n",
      "Initializing AWS Bedrock client.\n",
      "AWS Bedrock client initialized successfully.\n",
      "Starting evaluation of all models.\n",
      "Evaluating all models on the dataset.\n",
      "Evaluating facebook/bart-large-cnn...\n",
      "Evaluating model: facebook/bart-large-cnn\n",
      "Loading model: facebook/bart-large-cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for a given text.\n",
      "Calculating ROUGE scores.\n",
      "Generating summary for a given text.\n",
      "Calculating ROUGE scores.\n",
      "Evaluating google/pegasus-xsum...\n",
      "Evaluating model: google/pegasus-xsum\n",
      "Loading model: google/pegasus-xsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhire\\AppData\\Local\\Temp\\ipykernel_27768\\2588836411.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results = pd.concat([self.results, pd.DataFrame([new_result])], ignore_index=True)\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for a given text.\n",
      "Calculating ROUGE scores.\n",
      "Generating summary for a given text.\n",
      "Calculating ROUGE scores.\n",
      "Evaluating bedrock:amazon.titan-text-lite-v1...\n",
      "Evaluating model: bedrock:amazon.titan-text-lite-v1\n",
      "Loading model: bedrock:amazon.titan-text-lite-v1\n",
      "Generating summary for a given text.\n",
      "Generating summary using Amazon Bedrock model: bedrock:amazon.titan-text-lite-v1\n",
      "Successfully generated summary using Bedrock.\n",
      "Calculating ROUGE scores.\n",
      "Generating summary for a given text.\n",
      "Generating summary using Amazon Bedrock model: bedrock:amazon.titan-text-lite-v1\n",
      "Successfully generated summary using Bedrock.\n",
      "Calculating ROUGE scores.\n",
      "Visualizing evaluation results.\n",
      "Visualizing ROUGE scores for all models.\n",
      "Visualization is not available as matplotlib is not installed.\n",
      "                               Model   ROUGE-1   ROUGE-2   ROUGE-L\n",
      "0            facebook/bart-large-cnn  0.268231  0.086562  0.234596\n",
      "1                google/pegasus-xsum  0.253968  0.000000  0.216931\n",
      "2  bedrock:amazon.titan-text-lite-v1  0.187114  0.079089  0.149691\n"
     ]
    }
   ],
   "source": [
    "class LLMComparisonFramework:\n",
    "    def __init__(self, models, dataset, max_summary_length=100):\n",
    "        print(\"Initializing the framework with models and dataset.\")\n",
    "        self.models = models  # List of model names\n",
    "        self.dataset = dataset  # Dataset as a DataFrame with 'text' and 'reference_summary'\n",
    "        self.max_summary_length = max_summary_length\n",
    "        self.results = pd.DataFrame(columns=['Model', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'])\n",
    "        self.bedrock_client = self.initialize_bedrock_client()\n",
    "\n",
    "    def initialize_bedrock_client(self):\n",
    "        print(\"Initializing AWS Bedrock client.\")\n",
    "        try:\n",
    "            aws_session = session.Session()\n",
    "            client = aws_session.client(\n",
    "                'bedrock-runtime',\n",
    "                region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"),\n",
    "                aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "                aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "            )\n",
    "            print(\"AWS Bedrock client initialized successfully.\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize AWS Bedrock client: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        if model_name.startswith(\"bedrock:\"):\n",
    "            return None, model_name  # Bedrock models do not use tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        return tokenizer, model\n",
    "\n",
    "    def generate_summary(self, model, tokenizer, text):\n",
    "        print(\"Generating summary for a given text.\")\n",
    "        if isinstance(model, str) and model.startswith(\"bedrock:\"):\n",
    "            return self.generate_bedrock_summary(model, text)\n",
    "\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True)\n",
    "        outputs = model.generate(inputs, max_length=self.max_summary_length, num_beams=4)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def client(self, service_name):\n",
    "        print(f\"Creating client for service: {service_name}\")\n",
    "        import boto3\n",
    "        return boto3.client(service_name)\n",
    "\n",
    "    def generate_bedrock_summary(self, model_name, text):\n",
    "        print(f\"Generating summary using Amazon Bedrock model: {model_name}\")\n",
    "        model_id = model_name.split(\":\")[1]  # Extract Bedrock model ID\n",
    "        payload = {\n",
    "            \"inputText\": text,\n",
    "                    \"textGenerationConfig\": {\n",
    "                        \"maxTokenCount\": 4096,\n",
    "                        \"stopSequences\": [],\n",
    "                        \"temperature\": 0,\n",
    "                        \"topP\": 1\n",
    "                    }\n",
    "                    }\n",
    "        try:\n",
    "            response = self.bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "            print(\"Successfully generated summary using Bedrock.\")\n",
    "            return response[\"body\"].read().decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary with Bedrock model {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def calculate_rouge(self, predicted, reference):\n",
    "        print(\"Calculating ROUGE scores.\")\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(reference, predicted)\n",
    "        return {\n",
    "            'ROUGE-1': scores['rouge1'].fmeasure,\n",
    "            'ROUGE-2': scores['rouge2'].fmeasure,\n",
    "            'ROUGE-L': scores['rougeL'].fmeasure\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, model_name):\n",
    "        print(f\"Evaluating model: {model_name}\")\n",
    "        tokenizer, model = self.load_model(model_name)\n",
    "        rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "        for _, row in self.dataset.iterrows():\n",
    "            text = row['text']\n",
    "            reference_summary = row['reference_summary']\n",
    "\n",
    "            predicted_summary = self.generate_summary(model, tokenizer, text)\n",
    "            rouge_scores = self.calculate_rouge(predicted_summary, reference_summary)\n",
    "\n",
    "            rouge1_scores.append(rouge_scores['ROUGE-1'])\n",
    "            rouge2_scores.append(rouge_scores['ROUGE-2'])\n",
    "            rougeL_scores.append(rouge_scores['ROUGE-L'])\n",
    "\n",
    "        new_result = {\n",
    "            'Model': model_name,\n",
    "            'ROUGE-1': sum(rouge1_scores) / len(rouge1_scores),\n",
    "            'ROUGE-2': sum(rouge2_scores) / len(rouge2_scores),\n",
    "            'ROUGE-L': sum(rougeL_scores) / len(rougeL_scores)\n",
    "        }\n",
    "        self.results = pd.concat([self.results, pd.DataFrame([new_result])], ignore_index=True)\n",
    "\n",
    "    def evaluate_all_models(self):\n",
    "        print(\"Evaluating all models on the dataset.\")\n",
    "        for model_name in self.models:\n",
    "            print(f\"Evaluating {model_name}...\")\n",
    "            self.evaluate_model(model_name)\n",
    "\n",
    "    def visualize_results(self):\n",
    "        print(\"Visualizing ROUGE scores for all models.\")\n",
    "        print(\"Visualization is not available as matplotlib is not installed.\")\n",
    "        print(self.results)\n",
    "\n",
    "    def deploy_best_model(self):\n",
    "        print(\"Selecting and deploying the best model based on ROUGE-L score.\")\n",
    "        best_model_name = self.results.loc[self.results['ROUGE-L'].idxmax(), 'Model']\n",
    "        print(f\"Deploying best model: {best_model_name}\")\n",
    "        return self.load_model(best_model_name)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset with real-world text and summaries (e.g., news articles)\n",
    "    print(\"Creating example dataset.\")\n",
    "    data = {\n",
    "        'text': [\n",
    "            \"OpenAI has announced a new AI model that performs natural language processing tasks more efficiently.\",\n",
    "            \"NASA's Perseverance rover has collected new samples that provide insights into the possibility of past life on Mars.\"\n",
    "        ],\n",
    "        'reference_summary': [\n",
    "            \"OpenAI introduces a new efficient AI model for NLP.\",\n",
    "            \"NASA's rover collects samples hinting at past life on Mars.\"\n",
    "        ]\n",
    "    }\n",
    "    dataset = pd.DataFrame(data)\n",
    "\n",
    "    # Models to compare\n",
    "    print(\"Defining models to compare.\")\n",
    "    models = [\"facebook/bart-large-cnn\", \"google/pegasus-xsum\", \"bedrock:amazon.titan-text-lite-v1\"]\n",
    "\n",
    "    # Initialize framework\n",
    "    print(\"Initializing LLMComparisonFramework.\")\n",
    "    framework = LLMComparisonFramework(models, dataset)\n",
    "\n",
    "    # Evaluate models\n",
    "    print(\"Starting evaluation of all models.\")\n",
    "    framework.evaluate_all_models()\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"Visualizing evaluation results.\")\n",
    "    framework.visualize_results()\n",
    "\n",
    "    # Deploy the best model\n",
    "    # print(\"Deploying the best model.\")\n",
    "    # framework.deploy_best_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
